{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPBdCMotU4yKC1K7GYUL+/d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranagursoy/ECG-signal/blob/main/ECG-kaggledata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data"
      ],
      "metadata": {
        "id": "AHTeyrRErvXN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk8Sd7wio0-o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LfYpZ3uUo14C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/ColabNotebooks/dosya\""
      ],
      "metadata": {
        "id": "YEFOiSbLo3SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d erhmrai/ecg-image-data"
      ],
      "metadata": {
        "id": "X-VURrlPo85S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \\*.zip  && rm *.zip"
      ],
      "metadata": {
        "id": "tqH85WFbpQBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prep"
      ],
      "metadata": {
        "id": "NaYNd6xGrzCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_images_in_directory(directory):\n",
        "    image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
        "    image_counts = {}\n",
        "\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for dir_name in dirs:\n",
        "            subdir_path = os.path.join(root, dir_name)\n",
        "            count = len([file for file in os.listdir(subdir_path) if file.lower().endswith(image_extensions)])\n",
        "            image_counts[dir_name] = count\n",
        "\n",
        "    return image_counts\n",
        "\n",
        "def print_image_counts(directory, name):\n",
        "    image_counts = count_images_in_directory(directory)\n",
        "    total_images = sum(image_counts.values())\n",
        "\n",
        "    print(f\"\\n{name} Directory:\")\n",
        "    for dir_name, count in image_counts.items():\n",
        "        print(f\"{dir_name}: {count} images\")\n",
        "\n",
        "    return total_images\n",
        "\n",
        "train_directory = '/content/ECG_Image_data/train'\n",
        "test_directory = '/content/ECG_Image_data/test'\n",
        "\n",
        "train_image_count = print_image_counts(train_directory, \"Train\")\n",
        "test_image_count = print_image_counts(test_directory, \"Test\")\n",
        "\n",
        "total_images = train_image_count + test_image_count\n",
        "\n",
        "train_percentage = (train_image_count / total_images) * 100\n",
        "test_percentage = (test_image_count / total_images) * 100\n",
        "\n",
        "print(f\"\\nOverall Image Distribution:\")\n",
        "print(f\"Train: {train_image_count} images ({train_percentage:.2f}%)\")\n",
        "print(f\"Test: {test_image_count} images ({test_percentage:.2f}%)\")"
      ],
      "metadata": {
        "id": "RG9hzyh0pcdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_data(source_directory, train_directory, val_directory, test_directory, train_ratio=0.75, val_ratio=0.15, test_ratio=0.15):\n",
        "    for subdir in os.listdir(source_directory):\n",
        "        subdir_path = os.path.join(source_directory, subdir)\n",
        "\n",
        "        if os.path.isdir(subdir_path):\n",
        "            all_files = [f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))]\n",
        "\n",
        "            random.shuffle(all_files)\n",
        "            train_split = int(len(all_files) * train_ratio)\n",
        "            val_split = int(len(all_files) * val_ratio)\n",
        "\n",
        "            train_files = all_files[:train_split]\n",
        "            val_files = all_files[train_split:train_split + val_split]\n",
        "            test_files = all_files[train_split + val_split:]\n",
        "\n",
        "            train_subdir_path = os.path.join(train_directory, subdir)\n",
        "            val_subdir_path = os.path.join(val_directory, subdir)\n",
        "            test_subdir_path = os.path.join(test_directory, subdir)\n",
        "\n",
        "            os.makedirs(train_subdir_path, exist_ok=True)\n",
        "            os.makedirs(val_subdir_path, exist_ok=True)\n",
        "            os.makedirs(test_subdir_path, exist_ok=True)\n",
        "\n",
        "            for file in train_files:\n",
        "                shutil.move(os.path.join(subdir_path, file), os.path.join(train_subdir_path, file))\n",
        "\n",
        "            for file in val_files:\n",
        "                shutil.move(os.path.join(subdir_path, file), os.path.join(val_subdir_path, file))\n",
        "\n",
        "            for file in test_files:\n",
        "                shutil.move(os.path.join(subdir_path, file), os.path.join(test_subdir_path, file))\n",
        "\n",
        "\n",
        "source_directory = '/content/ECG_Image_data'\n",
        "train_directory = '/content/ECG_Image_data/train'\n",
        "val_directory = '/content/ECG_Image_data/val'\n",
        "test_directory = '/content/ECG_Image_data/test'\n",
        "\n",
        "split_data(source_directory, train_directory, val_directory, test_directory)"
      ],
      "metadata": {
        "id": "qLaCfwodpnAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_count = count_images_in_directory(\"/content/ECG_Image_data/train\")\n",
        "test_image_count = count_images_in_directory(\"/content/ECG_Image_data/new_test\")\n",
        "val_image_count = count_images_in_directory(\"/content/ECG_Image_data/val\")\n",
        "\n",
        "print(f\"Train: {train_image_count}\")\n",
        "print(f\"Test: {test_image_count}\")\n",
        "print(f\"Val: {val_image_count}\")"
      ],
      "metadata": {
        "id": "Ny5JJwDS_XsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def balance_and_split_data(source_directory, train_directory, val_directory, test_directory, max_files_per_class):\n",
        "    # Alt klasörler: train, val, test\n",
        "    source_train_dir = os.path.join(source_directory, 'train')\n",
        "    source_val_dir = os.path.join(source_directory, 'val')\n",
        "    source_test_dir = os.path.join(source_directory, 'test')\n",
        "\n",
        "    # Her sınıf için toplu dosya listesi\n",
        "    all_files = {}\n",
        "\n",
        "    # Her alt klasördeki dosyaları toplama\n",
        "    for data_type, data_dir in zip(['train', 'val', 'test'], [source_train_dir, source_val_dir, source_test_dir]):\n",
        "        for subdir in os.listdir(data_dir):\n",
        "            subdir_path = os.path.join(data_dir, subdir)\n",
        "            if os.path.isdir(subdir_path) and subdir != '.ipynb_checkpoints':\n",
        "                files = [os.path.join(subdir_path, f) for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))]\n",
        "                if subdir not in all_files:\n",
        "                    all_files[subdir] = []\n",
        "                all_files[subdir].extend(files)\n",
        "\n",
        "    # Belirtilen maksimum sayıda dosyayı train, val ve test klasörlerine kopyalama\n",
        "    for subdir, files in all_files.items():\n",
        "        random.shuffle(files)\n",
        "\n",
        "        max_train = max_files_per_class.get(subdir, {}).get('train', 0)\n",
        "        max_val = max_files_per_class.get(subdir, {}).get('val', 0)\n",
        "        max_test = max_files_per_class.get(subdir, {}).get('test', 0)\n",
        "\n",
        "        # Sadece belirli sayıda dosya alın\n",
        "        train_files = files[:max_train]\n",
        "        val_files = files[max_train:max_train + max_val]\n",
        "        test_files = files[max_train + max_val:max_train + max_val + max_test]\n",
        "\n",
        "        # Her klasör için hedef yollar\n",
        "        train_subdir_path = os.path.join(train_directory, subdir)\n",
        "        val_subdir_path = os.path.join(val_directory, subdir)\n",
        "        test_subdir_path = os.path.join(test_directory, subdir)\n",
        "\n",
        "        os.makedirs(train_subdir_path, exist_ok=True)\n",
        "        os.makedirs(val_subdir_path, exist_ok=True)\n",
        "        os.makedirs(test_subdir_path, exist_ok=True)\n",
        "\n",
        "        # Dosyaları ilgili klasörlere kopyalama\n",
        "        for file in train_files:\n",
        "            shutil.copy(file, os.path.join(train_subdir_path, os.path.basename(file)))\n",
        "\n",
        "        for file in val_files:\n",
        "            shutil.copy(file, os.path.join(val_subdir_path, os.path.basename(file)))\n",
        "\n",
        "        for file in test_files:\n",
        "            shutil.copy(file, os.path.join(test_subdir_path, os.path.basename(file)))\n",
        "\n",
        "# Ana kaynak dizin ve hedef dizinler\n",
        "source_directory = '/content/ECG_Image_data'\n",
        "train_directory = '/content/ECG_Image_data_balanced/train'\n",
        "val_directory = '/content/ECG_Image_data_balanced/val'\n",
        "test_directory = '/content/ECG_Image_data_balanced/test'\n",
        "\n",
        "# Her sınıf için farklı max_files_per_class değerlerini belirleme\n",
        "max_files_per_class = {\n",
        "    'F': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'N': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'V': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'S': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'Q': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'M': {'train': 800, 'val': 200, 'test': 150}\n",
        "}\n",
        "\n",
        "\n",
        "# Veriyi yeniden dengelemek ve kopyalamak\n",
        "balance_and_split_data(source_directory, train_directory, val_directory, test_directory, max_files_per_class=max_files_per_class)\n"
      ],
      "metadata": {
        "id": "xoqtX1tbAW0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def remove_ipynb_checkpoints(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        if '.ipynb_checkpoints' in dirs:\n",
        "            checkpoint_path = os.path.join(root, '.ipynb_checkpoints')\n",
        "            shutil.rmtree(checkpoint_path)\n",
        "            print(f\"Removed: {checkpoint_path}\")\n",
        "\n",
        "# Ana dizinler\n",
        "source_directory = '/content/ECG_Image_data'\n",
        "train_directory = '/content/ECG_Image_data_balanced/train'\n",
        "val_directory = '/content/ECG_Image_data_balanced/val'\n",
        "test_directory = '/content/ECG_Image_data_balanced/test'\n",
        "\n",
        "# Tüm dizinlerde .ipynb_checkpoints klasörlerini sil\n",
        "remove_ipynb_checkpoints(source_directory)\n",
        "remove_ipynb_checkpoints(train_directory)\n",
        "remove_ipynb_checkpoints(val_directory)\n",
        "remove_ipynb_checkpoints(test_directory)\n"
      ],
      "metadata": {
        "id": "vvqtZboVGBHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def find_corrupted_images(directory):\n",
        "    corrupted_files = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            try:\n",
        "                img = Image.open(file_path)\n",
        "                img.verify()  # Verify that it is, in fact, an image\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(f\"Corrupted image detected: {file_path}\")\n",
        "                corrupted_files.append(file_path)\n",
        "    return corrupted_files\n",
        "\n",
        "train_dir = '/content/ECG_Image_data_balanced/train'\n",
        "val_dir = '/content/ECG_Image_data_balanced/val'\n",
        "test_dir = '/content/ECG_Image_data_balanced/test'\n",
        "\n",
        "# Tüm veri setini kontrol edin\n",
        "corrupted_files = find_corrupted_images(train_dir) + find_corrupted_images(val_dir) + find_corrupted_images(test_dir)\n",
        "\n",
        "# Bozuk dosyaları temizle\n",
        "for file_path in corrupted_files:\n",
        "    os.remove(file_path)\n",
        "\n",
        "print(f\"{len(corrupted_files)} corrupted files removed.\")\n"
      ],
      "metadata": {
        "id": "rR5ySc1kamdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_count = count_images_in_directory(\"/content/ECG_Image_data_balanced/train\")\n",
        "test_image_count = count_images_in_directory(\"/content/ECG_Image_data_balanced/test\")\n",
        "val_image_count = count_images_in_directory(\"/content/ECG_Image_data_balanced/val\")\n",
        "\n",
        "print(f\"Train: {train_image_count}\")\n",
        "print(f\"Test: {test_image_count}\")\n",
        "print(f\"Val: {val_image_count}\")"
      ],
      "metadata": {
        "id": "rLiIQ-dEAZWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_count = print_image_counts(\"/content/ECG_Image_data_balanced/train\", \"Train\")\n",
        "test_image_count = print_image_counts(\"/content/ECG_Image_data_balanced/test\", \"Test\")\n",
        "val_image_count = print_image_counts(\"/content/ECG_Image_data_balanced/val\", \"Val\")\n",
        "\n",
        "total_images = train_image_count + test_image_count + val_image_count\n",
        "\n",
        "train_percentage = (train_image_count / total_images) * 100\n",
        "test_percentage = (test_image_count / total_images) * 100\n",
        "val_percentage = (val_image_count / total_images) * 100\n",
        "\n",
        "print(f\"Train: {train_image_count} images ({train_percentage:.2f}%)\")\n",
        "print(f\"Test: {test_image_count} images ({test_percentage:.2f}%)\")\n",
        "print(f\"Val: {val_image_count} images ({val_percentage:.2f}%)\")"
      ],
      "metadata": {
        "id": "S0PXO0hNAvsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Resmin yolu\n",
        "image_path = '/content/ECG_Image_data_balanced/train/F/F0.png'\n",
        "\n",
        "# Resmi aç\n",
        "img = Image.open(image_path)\n",
        "\n",
        "# Resmin boyutlarını al (genişlik, yükseklik)\n",
        "width, height = img.size\n",
        "\n",
        "print(f\"Resmin genişliği: {width} piksel\")\n",
        "print(f\"Resmin yüksekliği: {height} piksel\")\n"
      ],
      "metadata": {
        "id": "pSdBow1aVZbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Results"
      ],
      "metadata": {
        "id": "8YsjXRoXr4Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Enable GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"Using GPU devices: {gpus}\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# ImageDataGenerator for data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Directories for training, validation, and test sets\n",
        "train_dir = '/content/ECG_Image_data_balanced/train'\n",
        "val_dir = '/content/ECG_Image_data_balanced/val'\n",
        "test_dir = '/content/ECG_Image_data_balanced/test'\n",
        "\n",
        "# Flow data from directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(432, 288),  # Resmin genişliği: 432 piksel, yüksekliği: 288 piksel\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = test_val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(432, 288),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_val_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(432, 288),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # Important for getting correct labels when predicting\n",
        ")\n",
        "\n",
        "# Load VGG16 model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(432, 288, 3))\n",
        "\n",
        "# Freeze the first 15 layers of VGG16 to prevent them from being trained\n",
        "for layer in base_model.layers[:10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Keep the rest of the layers trainable\n",
        "for layer in base_model.layers[10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Add custom layers on top\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0001)  # VGG16 için daha düşük bir öğrenme oranı\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Directory for results\n",
        "results_dir = '/content/drive/MyDrive/ECG-Results/training_results_vgg16_6_classes_1'\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "checkpoint_path = os.path.join(results_dir, 'best_model_vgg16.keras')\n",
        "checkpoint = ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, mode='min')\n",
        "\n",
        "# Learning rate reduction\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Save the training history\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.to_csv(os.path.join(results_dir, 'training_history_vgg16.csv'))\n",
        "\n",
        "# Evaluate the model on train data\n",
        "train_generator.reset()\n",
        "Y_train_pred = model.predict(train_generator)\n",
        "Y_train_pred_classes = np.argmax(Y_train_pred, axis=1)\n",
        "y_train_classes = train_generator.classes  # True labels\n",
        "\n",
        "# Confusion matrix and classification report for train data\n",
        "confusion_mtx_train = confusion_matrix(y_train_classes, Y_train_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx_train, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Train Data)')\n",
        "plt.savefig(os.path.join(results_dir, 'confusion_matrix_train_vgg16.png'))\n",
        "plt.close()\n",
        "\n",
        "report_train = classification_report(y_train_classes, Y_train_pred_classes, target_names=list(train_generator.class_indices.keys()))\n",
        "with open(os.path.join(results_dir, 'classification_report_train_vgg16.txt'), 'w') as f:\n",
        "    f.write(report_train)\n",
        "\n",
        "# Evaluate the model on validation data\n",
        "val_generator.reset()\n",
        "Y_val_pred = model.predict(val_generator)\n",
        "Y_val_pred_classes = np.argmax(Y_val_pred, axis=1)\n",
        "y_val_classes = val_generator.classes  # True labels\n",
        "\n",
        "# Confusion matrix and classification report for validation data\n",
        "confusion_mtx_val = confusion_matrix(y_val_classes, Y_val_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx_val, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Validation Data)')\n",
        "plt.savefig(os.path.join(results_dir, 'confusion_matrix_val_vgg16.png'))\n",
        "plt.close()\n",
        "\n",
        "report_val = classification_report(y_val_classes, Y_val_pred_classes, target_names=list(val_generator.class_indices.keys()))\n",
        "with open(os.path.join(results_dir, 'classification_report_val_vgg16.txt'), 'w') as f:\n",
        "    f.write(report_val)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_generator.reset()\n",
        "Y_pred = model.predict(test_generator)\n",
        "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "y_test_classes = test_generator.classes  # True labels\n",
        "\n",
        "# Handle the case where there are missing classes in predictions\n",
        "unique_true_labels = np.unique(y_test_classes)\n",
        "unique_pred_labels = np.unique(Y_pred_classes)\n",
        "\n",
        "# Tüm sınıflar (6 sınıf olduğunu varsayıyoruz)\n",
        "all_classes = np.array(range(len(test_generator.class_indices)))\n",
        "\n",
        "# Eksik sınıfları doldur\n",
        "missing_true_labels = np.setdiff1d(all_classes, unique_true_labels)\n",
        "missing_pred_labels = np.setdiff1d(all_classes, unique_pred_labels)\n",
        "\n",
        "# Gerçek sınıflara eksik sınıfları ekle\n",
        "for label in missing_true_labels:\n",
        "    y_test_classes = np.append(y_test_classes, label)\n",
        "    Y_pred_classes = np.append(Y_pred_classes, label)\n",
        "\n",
        "# Tahmin edilen sınıflara eksik sınıfları ekle\n",
        "for label in missing_pred_labels:\n",
        "    Y_pred_classes = np.append(Y_pred_classes, label)\n",
        "    y_test_classes = np.append(y_test_classes, label)\n",
        "\n",
        "# Confusion matrix and classification report for test data\n",
        "confusion_mtx = confusion_matrix(y_test_classes, Y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Test Data)')\n",
        "plt.savefig(os.path.join(results_dir, 'confusion_matrix_vgg16.png'))\n",
        "plt.close()\n",
        "\n",
        "report = classification_report(y_test_classes, Y_pred_classes, target_names=list(test_generator.class_indices.keys()))\n",
        "with open(os.path.join(results_dir, 'classification_report_vgg16.txt'), 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(\"Model training and evaluation complete. Results saved in 'training_results_vgg16_6_classes' directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "tMEP13hlqesc",
        "outputId": "e5244cba-1a59-4c39-fc9b-b31f2961b024"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/ECG_Image_data_balanced/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6aba4f417634>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Flow data from directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m train_generator = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m432\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m288\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Resmin genişliği: 432 piksel, yüksekliği: 288 piksel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/ECG_Image_data_balanced/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mRMW03cxBR-d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}