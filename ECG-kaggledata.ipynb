{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranagursoy/ECG-signal/blob/main/ECG-kaggledata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHTeyrRErvXN"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wk8Sd7wio0-o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LfYpZ3uUo14C"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEFOiSbLo3SS"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/ColabNotebooks/dosya\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-VURrlPo85S"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d erhmrai/ecg-image-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqH85WFbpQBU"
      },
      "outputs": [],
      "source": [
        "!unzip /*.zip  && rm *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaYNd6xGrzCQ"
      },
      "source": [
        "# Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RG9hzyh0pcdR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Directory:\n",
            "F: 642 images\n",
            "M: 8405 images\n",
            "N: 75709 images\n",
            "Q: 6431 images\n",
            "S: 2223 images\n",
            "V: 5789 images\n",
            "\n",
            "Test Directory:\n",
            "F: 161 images\n",
            "M: 2101 images\n",
            "N: 18926 images\n",
            "Q: 1608 images\n",
            "S: 556 images\n",
            "V: 1447 images\n",
            "\n",
            "Overall Image Distribution:\n",
            "Train: 99199 images (80.00%)\n",
            "Test: 24799 images (20.00%)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def count_images_in_directory(directory):\n",
        "    image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
        "    image_counts = {}\n",
        "\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for dir_name in dirs:\n",
        "            subdir_path = os.path.join(root, dir_name)\n",
        "            count = len([file for file in os.listdir(subdir_path) if file.lower().endswith(image_extensions)])\n",
        "            image_counts[dir_name] = count\n",
        "\n",
        "    return image_counts\n",
        "\n",
        "def print_image_counts(directory, name):\n",
        "    image_counts = count_images_in_directory(directory)\n",
        "    total_images = sum(image_counts.values())\n",
        "\n",
        "    print(f\"\\n{name} Directory:\")\n",
        "    for dir_name, count in image_counts.items():\n",
        "        print(f\"{dir_name}: {count} images\")\n",
        "\n",
        "    return total_images\n",
        "\n",
        "train_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data/train'\n",
        "test_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data/test'\n",
        "\n",
        "train_image_count = print_image_counts(train_directory, \"Train\")\n",
        "test_image_count = print_image_counts(test_directory, \"Test\")\n",
        "\n",
        "total_images = train_image_count + test_image_count\n",
        "\n",
        "train_percentage = (train_image_count / total_images) * 100\n",
        "test_percentage = (test_image_count / total_images) * 100\n",
        "\n",
        "print(f\"\\nOverall Image Distribution:\")\n",
        "print(f\"Train: {train_image_count} images ({train_percentage:.2f}%)\")\n",
        "print(f\"Test: {test_image_count} images ({test_percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qLaCfwodpnAE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_data(source_directory, train_directory, val_directory, test_directory, train_ratio=0.75, val_ratio=0.15, test_ratio=0.15):\n",
        "    for subdir in os.listdir(source_directory):\n",
        "        subdir_path = os.path.join(source_directory, subdir)\n",
        "\n",
        "        if os.path.isdir(subdir_path):\n",
        "            all_files = [f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))]\n",
        "\n",
        "            random.shuffle(all_files)\n",
        "            train_split = int(len(all_files) * train_ratio)\n",
        "            val_split = int(len(all_files) * val_ratio)\n",
        "\n",
        "            train_files = all_files[:train_split]\n",
        "            val_files = all_files[train_split:train_split + val_split]\n",
        "            test_files = all_files[train_split + val_split:]\n",
        "\n",
        "            train_subdir_path = os.path.join(train_directory, subdir)\n",
        "            val_subdir_path = os.path.join(val_directory, subdir)\n",
        "            test_subdir_path = os.path.join(test_directory, subdir)\n",
        "\n",
        "            os.makedirs(train_subdir_path, exist_ok=True)\n",
        "            os.makedirs(val_subdir_path, exist_ok=True)\n",
        "            os.makedirs(test_subdir_path, exist_ok=True)\n",
        "\n",
        "            for file in train_files:\n",
        "                shutil.move(os.path.join(subdir_path, file), os.path.join(train_subdir_path, file))\n",
        "\n",
        "            for file in val_files:\n",
        "                shutil.move(os.path.join(subdir_path, file), os.path.join(val_subdir_path, file))\n",
        "\n",
        "            for file in test_files:\n",
        "                shutil.move(os.path.join(subdir_path, file), os.path.join(test_subdir_path, file))\n",
        "\n",
        "\n",
        "source_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data'\n",
        "train_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data/train'\n",
        "val_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data/val'\n",
        "test_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data/new_test'\n",
        "\n",
        "split_data(source_directory, train_directory, val_directory, test_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ny5JJwDS_XsW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: {'F': 642, 'M': 8405, 'N': 75709, 'Q': 6431, 'S': 2223, 'test': 0, 'train': 0, 'V': 5789, 'val': 0}\n",
            "Test: {'F': 161, 'M': 2101, 'N': 18926, 'Q': 1608, 'S': 556, 'V': 1447}\n",
            "Val: {'new_test': 0, 'test': 0, 'train': 0, 'val': 0}\n"
          ]
        }
      ],
      "source": [
        "train_image_count = count_images_in_directory(\"C:/Users/asil_/Downloads/archive/ECG_Image_data/train\")\n",
        "test_image_count = count_images_in_directory(\"C:/Users/asil_/Downloads/archive/ECG_Image_data/test\")\n",
        "val_image_count = count_images_in_directory(\"C:/Users/asil_/Downloads/archive/ECG_Image_data/val\")\n",
        "\n",
        "print(f\"Train: {train_image_count}\")\n",
        "print(f\"Test: {test_image_count}\")\n",
        "print(f\"Val: {val_image_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xoqtX1tbAW0m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def balance_and_split_data(source_directory, train_directory, val_directory, test_directory, max_files_per_class):\n",
        "    # Alt klas√∂rler: train, val, test\n",
        "    source_train_dir = os.path.join(source_directory, 'train')\n",
        "    source_val_dir = os.path.join(source_directory, 'val')\n",
        "    source_test_dir = os.path.join(source_directory, 'test')\n",
        "\n",
        "    # Her sƒ±nƒ±f i√ßin toplu dosya listesi\n",
        "    all_files = {}\n",
        "\n",
        "    # Her alt klas√∂rdeki dosyalarƒ± toplama\n",
        "    for data_type, data_dir in zip(['train', 'val', 'test'], [source_train_dir, source_val_dir, source_test_dir]):\n",
        "        for subdir in os.listdir(data_dir):\n",
        "            subdir_path = os.path.join(data_dir, subdir)\n",
        "            if os.path.isdir(subdir_path) and subdir != '.ipynb_checkpoints':\n",
        "                files = [os.path.join(subdir_path, f) for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))]\n",
        "                if subdir not in all_files:\n",
        "                    all_files[subdir] = []\n",
        "                all_files[subdir].extend(files)\n",
        "\n",
        "    # Belirtilen maksimum sayƒ±da dosyayƒ± train, val ve test klas√∂rlerine kopyalama\n",
        "    for subdir, files in all_files.items():\n",
        "        random.shuffle(files)\n",
        "\n",
        "        max_train = max_files_per_class.get(subdir, {}).get('train', 0)\n",
        "        max_val = max_files_per_class.get(subdir, {}).get('val', 0)\n",
        "        max_test = max_files_per_class.get(subdir, {}).get('test', 0)\n",
        "\n",
        "        # Sadece belirli sayƒ±da dosya alƒ±n\n",
        "        train_files = files[:max_train]\n",
        "        val_files = files[max_train:max_train + max_val]\n",
        "        test_files = files[max_train + max_val:max_train + max_val + max_test]\n",
        "\n",
        "        # Her klas√∂r i√ßin hedef yollar\n",
        "        train_subdir_path = os.path.join(train_directory, subdir)\n",
        "        val_subdir_path = os.path.join(val_directory, subdir)\n",
        "        test_subdir_path = os.path.join(test_directory, subdir)\n",
        "\n",
        "        os.makedirs(train_subdir_path, exist_ok=True)\n",
        "        os.makedirs(val_subdir_path, exist_ok=True)\n",
        "        os.makedirs(test_subdir_path, exist_ok=True)\n",
        "\n",
        "        # Dosyalarƒ± ilgili klas√∂rlere kopyalama\n",
        "        for file in train_files:\n",
        "            shutil.copy(file, os.path.join(train_subdir_path, os.path.basename(file)))\n",
        "\n",
        "        for file in val_files:\n",
        "            shutil.copy(file, os.path.join(val_subdir_path, os.path.basename(file)))\n",
        "\n",
        "        for file in test_files:\n",
        "            shutil.copy(file, os.path.join(test_subdir_path, os.path.basename(file)))\n",
        "\n",
        "# Ana kaynak dizin ve hedef dizinler\n",
        "source_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data'\n",
        "train_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data_balanced/train'\n",
        "val_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data_balanced/val'\n",
        "test_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data_balanced/test'\n",
        "\n",
        "# Her sƒ±nƒ±f i√ßin farklƒ± max_files_per_class deƒüerlerini belirleme\n",
        "max_files_per_class = {\n",
        "    'F': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'N': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'V': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'S': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'Q': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'M': {'train': 800, 'val': 200, 'test': 150}\n",
        "}\n",
        "\n",
        "\n",
        "# Veriyi yeniden dengelemek ve kopyalamak\n",
        "balance_and_split_data(source_directory, train_directory, val_directory, test_directory, max_files_per_class=max_files_per_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vvqtZboVGBHi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def remove_ipynb_checkpoints(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        if '.ipynb_checkpoints' in dirs:\n",
        "            checkpoint_path = os.path.join(root, '.ipynb_checkpoints')\n",
        "            shutil.rmtree(checkpoint_path)\n",
        "            print(f\"Removed: {checkpoint_path}\")\n",
        "\n",
        "# Ana dizinler\n",
        "source_directory = '/C:/Users/asil_/Downloads/archive/ECG_Image_data'\n",
        "train_directory = '/C:/Users/asil_/Downloads/archive/ECG_Image_data_balanced/train'\n",
        "val_directory = '/C:/Users/asil_/Downloads/archive/ECG_Image_data_balanced/val'\n",
        "test_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data_balanced/test'\n",
        "\n",
        "# T√ºm dizinlerde .ipynb_checkpoints klas√∂rlerini sil\n",
        "remove_ipynb_checkpoints(source_directory)\n",
        "remove_ipynb_checkpoints(train_directory)\n",
        "remove_ipynb_checkpoints(val_directory)\n",
        "remove_ipynb_checkpoints(test_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rR5ySc1kamdq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 corrupted files removed.\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def find_corrupted_images(directory):\n",
        "    corrupted_files = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            try:\n",
        "                img = Image.open(file_path)\n",
        "                img.verify()  # Verify that it is, in fact, an image\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(f\"Corrupted image detected: {file_path}\")\n",
        "                corrupted_files.append(file_path)\n",
        "    return corrupted_files\n",
        "\n",
        "train_dir = train_directory\n",
        "val_dir = val_directory \n",
        "test_dir = test_directory \n",
        "\n",
        "# T√ºm veri setini kontrol edin\n",
        "corrupted_files = find_corrupted_images(train_dir) + find_corrupted_images(val_dir) + find_corrupted_images(test_dir)\n",
        "\n",
        "# Bozuk dosyalarƒ± temizle\n",
        "for file_path in corrupted_files:\n",
        "    os.remove(file_path)\n",
        "\n",
        "print(f\"{len(corrupted_files)} corrupted files removed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "rLiIQ-dEAZWS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: {}\n",
            "Test: {'F': 0, 'M': 298, 'N': 300, 'new_test': 0, 'Q': 296, 'S': 293, 'test': 0, 'train': 0, 'V': 299, 'val': 0}\n",
            "Val: {}\n"
          ]
        }
      ],
      "source": [
        "train_image_count = count_images_in_directory(train_dir)\n",
        "test_image_count = count_images_in_directory(test_dir)\n",
        "val_image_count = count_images_in_directory(val_dir)\n",
        "\n",
        "print(f\"Train: {train_image_count}\")\n",
        "print(f\"Test: {test_image_count}\")\n",
        "print(f\"Val: {val_image_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "S0PXO0hNAvsU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Directory:\n",
            "\n",
            "Test Directory:\n",
            "F: 0 images\n",
            "M: 298 images\n",
            "N: 300 images\n",
            "new_test: 0 images\n",
            "Q: 296 images\n",
            "S: 293 images\n",
            "test: 0 images\n",
            "train: 0 images\n",
            "V: 299 images\n",
            "val: 0 images\n",
            "\n",
            "Val Directory:\n",
            "Train: 0 images (0.00%)\n",
            "Test: 1486 images (100.00%)\n",
            "Val: 0 images (0.00%)\n"
          ]
        }
      ],
      "source": [
        "train_image_count = print_image_counts(train_dir, \"Train\")\n",
        "test_image_count = print_image_counts(test_dir, \"Test\")\n",
        "val_image_count = print_image_counts(val_dir, \"Val\")\n",
        "\n",
        "total_images = train_image_count + test_image_count + val_image_count\n",
        "\n",
        "train_percentage = (train_image_count / total_images) * 100\n",
        "test_percentage = (test_image_count / total_images) * 100\n",
        "val_percentage = (val_image_count / total_images) * 100\n",
        "\n",
        "print(f\"Train: {train_image_count} images ({train_percentage:.2f}%)\")\n",
        "print(f\"Test: {test_image_count} images ({test_percentage:.2f}%)\")\n",
        "print(f\"Val: {val_image_count} images ({val_percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSdBow1aVZbY"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Resmin yolu\n",
        "image_path = '/content/ECG_Image_data_balanced/train/F/F0.png'\n",
        "\n",
        "# Resmi a√ß\n",
        "img = Image.open(image_path)\n",
        "\n",
        "# Resmin boyutlarƒ±nƒ± al (geni≈ülik, y√ºkseklik)\n",
        "width, height = img.size\n",
        "\n",
        "print(f\"Resmin geni≈üliƒüi: {width} piksel\")\n",
        "print(f\"Resmin y√ºksekliƒüi: {height} piksel\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YsjXRoXr4Q6"
      },
      "source": [
        "# Train & Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "tMEP13hlqesc",
        "outputId": "e5244cba-1a59-4c39-fc9b-b31f2961b024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_dir' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 37>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m test_val_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Directories for training, validation, and test sets\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dir\u001b[49m\n\u001b[0;32m     38\u001b[0m val_dir \u001b[38;5;241m=\u001b[39m val_dir\n\u001b[0;32m     39\u001b[0m test_dir \u001b[38;5;241m=\u001b[39m test_dir\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_dir' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Enable GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"Using GPU devices: {gpus}\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# ImageDataGenerator for data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Directories for training, validation, and test sets\n",
        "train_dir = train_dir\n",
        "val_dir = val_dir\n",
        "test_dir = test_dir\n",
        "\n",
        "# Flow data from directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(432, 224),  # Resmin geni≈üliƒüi: 432 piksel, y√ºksekliƒüi: 288 piksel\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = test_val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(432, 288),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_val_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(432, 288),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # Important for getting correct labels when predicting\n",
        ")\n",
        "\n",
        "# Load VGG16 model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(432, 288, 3))\n",
        "\n",
        "# Freeze the first 15 layers of VGG16 to prevent them from being trained\n",
        "for layer in base_model.layers[:10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Keep the rest of the layers trainable\n",
        "for layer in base_model.layers[10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Add custom layers on top\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0001)  # VGG16 i√ßin daha d√º≈ü√ºk bir √∂ƒürenme oranƒ±\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Directory for results\n",
        "results_dir = 'training_results_vgg16_6_classes_1'\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "checkpoint_path = os.path.join(results_dir, 'best_model_vgg16.keras')\n",
        "checkpoint = ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, mode='min')\n",
        "\n",
        "# Learning rate reduction\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Save the training history\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.to_csv(os.path.join(results_dir, 'training_history_vgg16.csv'))\n",
        "\n",
        "# Evaluate the model on train data\n",
        "train_generator.reset()\n",
        "Y_train_pred = model.predict(train_generator)\n",
        "Y_train_pred_classes = np.argmax(Y_train_pred, axis=1)\n",
        "y_train_classes = train_generator.classes  # True labels\n",
        "\n",
        "# Confusion matrix and classification report for train data\n",
        "confusion_mtx_train = confusion_matrix(y_train_classes, Y_train_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx_train, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Train Data)')\n",
        "plt.savefig(os.path.join(results_dir, 'confusion_matrix_train_vgg16.png'))\n",
        "plt.close()\n",
        "\n",
        "report_train = classification_report(y_train_classes, Y_train_pred_classes, target_names=list(train_generator.class_indices.keys()))\n",
        "with open(os.path.join(results_dir, 'classification_report_train_vgg16.txt'), 'w') as f:\n",
        "    f.write(report_train)\n",
        "\n",
        "# Evaluate the model on validation data\n",
        "val_generator.reset()\n",
        "Y_val_pred = model.predict(val_generator)\n",
        "Y_val_pred_classes = np.argmax(Y_val_pred, axis=1)\n",
        "y_val_classes = val_generator.classes  # True labels\n",
        "\n",
        "# Confusion matrix and classification report for validation data\n",
        "confusion_mtx_val = confusion_matrix(y_val_classes, Y_val_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx_val, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Validation Data)')\n",
        "plt.savefig(os.path.join(results_dir, 'confusion_matrix_val_vgg16.png'))\n",
        "plt.close()\n",
        "\n",
        "report_val = classification_report(y_val_classes, Y_val_pred_classes, target_names=list(val_generator.class_indices.keys()))\n",
        "with open(os.path.join(results_dir, 'classification_report_val_vgg16.txt'), 'w') as f:\n",
        "    f.write(report_val)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_generator.reset()\n",
        "Y_pred = model.predict(test_generator)\n",
        "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "y_test_classes = test_generator.classes  # True labels\n",
        "\n",
        "# Handle the case where there are missing classes in predictions\n",
        "unique_true_labels = np.unique(y_test_classes)\n",
        "unique_pred_labels = np.unique(Y_pred_classes)\n",
        "\n",
        "# T√ºm sƒ±nƒ±flar (6 sƒ±nƒ±f olduƒüunu varsayƒ±yoruz)\n",
        "all_classes = np.array(range(len(test_generator.class_indices)))\n",
        "\n",
        "# Eksik sƒ±nƒ±flarƒ± doldur\n",
        "missing_true_labels = np.setdiff1d(all_classes, unique_true_labels)\n",
        "missing_pred_labels = np.setdiff1d(all_classes, unique_pred_labels)\n",
        "\n",
        "# Ger√ßek sƒ±nƒ±flara eksik sƒ±nƒ±flarƒ± ekle\n",
        "for label in missing_true_labels:\n",
        "    y_test_classes = np.append(y_test_classes, label)\n",
        "    Y_pred_classes = np.append(Y_pred_classes, label)\n",
        "\n",
        "# Tahmin edilen sƒ±nƒ±flara eksik sƒ±nƒ±flarƒ± ekle\n",
        "for label in missing_pred_labels:\n",
        "    Y_pred_classes = np.append(Y_pred_classes, label)\n",
        "    y_test_classes = np.append(y_test_classes, label)\n",
        "\n",
        "# Confusion matrix and classification report for test data\n",
        "confusion_mtx = confusion_matrix(y_test_classes, Y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Test Data)')\n",
        "plt.savefig(os.path.join(results_dir, 'confusion_matrix_vgg16.png'))\n",
        "plt.close()\n",
        "\n",
        "report = classification_report(y_test_classes, Y_pred_classes, target_names=list(test_generator.class_indices.keys()))\n",
        "with open(os.path.join(results_dir, 'classification_report_vgg16.txt'), 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(\"Model training and evaluation complete. Results saved in 'training_results_vgg16_6_classes' directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRMW03cxBR-d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "authorship_tag": "ABX9TyPBdCMotU4yKC1K7GYUL+/d",
      "gpuType": "V28",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
