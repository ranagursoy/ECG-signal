{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranagursoy/ECG-signal/blob/main/ECG-kaggledata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHTeyrRErvXN"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wk8Sd7wio0-o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LfYpZ3uUo14C"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEFOiSbLo3SS"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/ColabNotebooks/dosya\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-VURrlPo85S"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d erhmrai/ecg-image-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqH85WFbpQBU"
      },
      "outputs": [],
      "source": [
        "!unzip /*.zip  && rm *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaYNd6xGrzCQ"
      },
      "source": [
        "# Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RG9hzyh0pcdR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Directory:\n",
            "F: 642 images\n",
            "M: 8405 images\n",
            "N: 75709 images\n",
            "Q: 6431 images\n",
            "S: 2223 images\n",
            "V: 5789 images\n",
            "\n",
            "Test Directory:\n",
            "F: 161 images\n",
            "M: 2101 images\n",
            "N: 18926 images\n",
            "Q: 1608 images\n",
            "S: 556 images\n",
            "V: 1447 images\n",
            "\n",
            "Overall Image Distribution:\n",
            "Train: 99199 images (80.00%)\n",
            "Test: 24799 images (20.00%)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def count_images_in_directory(directory):\n",
        "    image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
        "    image_counts = {}\n",
        "\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for dir_name in dirs:\n",
        "            subdir_path = os.path.join(root, dir_name)\n",
        "            count = len([file for file in os.listdir(subdir_path) if file.lower().endswith(image_extensions)])\n",
        "            image_counts[dir_name] = count\n",
        "\n",
        "    return image_counts\n",
        "\n",
        "def print_image_counts(directory, name):\n",
        "    image_counts = count_images_in_directory(directory)\n",
        "    total_images = sum(image_counts.values())\n",
        "\n",
        "    print(f\"\\n{name} Directory:\")\n",
        "    for dir_name, count in image_counts.items():\n",
        "        print(f\"{dir_name}: {count} images\")\n",
        "\n",
        "    return total_images\n",
        "\n",
        "train_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data/train'\n",
        "test_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data/test'\n",
        "\n",
        "train_image_count = print_image_counts(train_directory, \"Train\")\n",
        "test_image_count = print_image_counts(test_directory, \"Test\")\n",
        "\n",
        "total_images = train_image_count + test_image_count\n",
        "\n",
        "train_percentage = (train_image_count / total_images) * 100\n",
        "test_percentage = (test_image_count / total_images) * 100\n",
        "\n",
        "print(f\"\\nOverall Image Distribution:\")\n",
        "print(f\"Train: {train_image_count} images ({train_percentage:.2f}%)\")\n",
        "print(f\"Test: {test_image_count} images ({test_percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qLaCfwodpnAE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_data(source_directory, train_directory, val_directory, test_directory, train_ratio=0.75, val_ratio=0.15, test_ratio=0.15):\n",
        "    for subdir in os.listdir(source_directory):\n",
        "        subdir_path = os.path.join(source_directory, subdir)\n",
        "\n",
        "        if os.path.isdir(subdir_path):\n",
        "            all_files = [f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))]\n",
        "\n",
        "            random.shuffle(all_files)\n",
        "            train_split = int(len(all_files) * train_ratio)\n",
        "            val_split = int(len(all_files) * val_ratio)\n",
        "\n",
        "            train_files = all_files[:train_split]\n",
        "            val_files = all_files[train_split:train_split + val_split]\n",
        "            test_files = all_files[train_split + val_split:]\n",
        "\n",
        "            train_subdir_path = os.path.join(train_directory, subdir)\n",
        "            val_subdir_path = os.path.join(val_directory, subdir)\n",
        "            test_subdir_path = os.path.join(test_directory, subdir)\n",
        "\n",
        "            os.makedirs(train_subdir_path, exist_ok=True)\n",
        "            os.makedirs(val_subdir_path, exist_ok=True)\n",
        "            os.makedirs(test_subdir_path, exist_ok=True)\n",
        "\n",
        "            for file in train_files:\n",
        "                shutil.move(os.path.join(subdir_path, file), os.path.join(train_subdir_path, file))\n",
        "\n",
        "            for file in val_files:\n",
        "                shutil.move(os.path.join(subdir_path, file), os.path.join(val_subdir_path, file))\n",
        "\n",
        "            for file in test_files:\n",
        "                shutil.move(os.path.join(subdir_path, file), os.path.join(test_subdir_path, file))\n",
        "\n",
        "\n",
        "source_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data'\n",
        "train_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data/train'\n",
        "val_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data/val'\n",
        "test_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data/new_test'\n",
        "\n",
        "split_data(source_directory, train_directory, val_directory, test_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ny5JJwDS_XsW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: {'F': 642, 'M': 8405, 'N': 75709, 'Q': 6431, 'S': 2223, 'test': 0, 'train': 0, 'V': 5789, 'val': 0}\n",
            "Test: {'F': 161, 'M': 2101, 'N': 18926, 'Q': 1608, 'S': 556, 'V': 1447}\n",
            "Val: {'new_test': 0, 'test': 0, 'train': 0, 'val': 0}\n"
          ]
        }
      ],
      "source": [
        "train_image_count = count_images_in_directory(\"C:/Users/asil_/Downloads/archive/ECG_Image_data/train\")\n",
        "test_image_count = count_images_in_directory(\"C:/Users/asil_/Downloads/archive/ECG_Image_data/test\")\n",
        "val_image_count = count_images_in_directory(\"C:/Users/asil_/Downloads/archive/ECG_Image_data/val\")\n",
        "\n",
        "print(f\"Train: {train_image_count}\")\n",
        "print(f\"Test: {test_image_count}\")\n",
        "print(f\"Val: {val_image_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xoqtX1tbAW0m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def balance_and_split_data(source_directory, train_directory, val_directory, test_directory, max_files_per_class):\n",
        "    # Alt klasörler: train, val, test\n",
        "    source_train_dir = os.path.join(source_directory, 'train')\n",
        "    source_val_dir = os.path.join(source_directory, 'val')\n",
        "    source_test_dir = os.path.join(source_directory, 'test')\n",
        "\n",
        "    # Her sınıf için toplu dosya listesi\n",
        "    all_files = {}\n",
        "\n",
        "    # Her alt klasördeki dosyaları toplama\n",
        "    for data_type, data_dir in zip(['train', 'val', 'test'], [source_train_dir, source_val_dir, source_test_dir]):\n",
        "        for subdir in os.listdir(data_dir):\n",
        "            subdir_path = os.path.join(data_dir, subdir)\n",
        "            if os.path.isdir(subdir_path) and subdir != '.ipynb_checkpoints':\n",
        "                files = [os.path.join(subdir_path, f) for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))]\n",
        "                if subdir not in all_files:\n",
        "                    all_files[subdir] = []\n",
        "                all_files[subdir].extend(files)\n",
        "\n",
        "    # Belirtilen maksimum sayıda dosyayı train, val ve test klasörlerine kopyalama\n",
        "    for subdir, files in all_files.items():\n",
        "        random.shuffle(files)\n",
        "\n",
        "        max_train = max_files_per_class.get(subdir, {}).get('train', 0)\n",
        "        max_val = max_files_per_class.get(subdir, {}).get('val', 0)\n",
        "        max_test = max_files_per_class.get(subdir, {}).get('test', 0)\n",
        "\n",
        "        # Sadece belirli sayıda dosya alın\n",
        "        train_files = files[:max_train]\n",
        "        val_files = files[max_train:max_train + max_val]\n",
        "        test_files = files[max_train + max_val:max_train + max_val + max_test]\n",
        "\n",
        "        # Her klasör için hedef yollar\n",
        "        train_subdir_path = os.path.join(train_directory, subdir)\n",
        "        val_subdir_path = os.path.join(val_directory, subdir)\n",
        "        test_subdir_path = os.path.join(test_directory, subdir)\n",
        "\n",
        "        os.makedirs(train_subdir_path, exist_ok=True)\n",
        "        os.makedirs(val_subdir_path, exist_ok=True)\n",
        "        os.makedirs(test_subdir_path, exist_ok=True)\n",
        "\n",
        "        # Dosyaları ilgili klasörlere kopyalama\n",
        "        for file in train_files:\n",
        "            shutil.copy(file, os.path.join(train_subdir_path, os.path.basename(file)))\n",
        "\n",
        "        for file in val_files:\n",
        "            shutil.copy(file, os.path.join(val_subdir_path, os.path.basename(file)))\n",
        "\n",
        "        for file in test_files:\n",
        "            shutil.copy(file, os.path.join(test_subdir_path, os.path.basename(file)))\n",
        "\n",
        "# Ana kaynak dizin ve hedef dizinler\n",
        "source_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data'\n",
        "train_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data_balanced/train'\n",
        "val_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data_balanced/val'\n",
        "test_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data_balanced/test'\n",
        "\n",
        "# Her sınıf için farklı max_files_per_class değerlerini belirleme\n",
        "max_files_per_class = {\n",
        "    'F': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'N': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'V': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'S': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'Q': {'train': 800, 'val': 200, 'test': 150},\n",
        "    'M': {'train': 800, 'val': 200, 'test': 150}\n",
        "}\n",
        "\n",
        "\n",
        "# Veriyi yeniden dengelemek ve kopyalamak\n",
        "balance_and_split_data(source_directory, train_directory, val_directory, test_directory, max_files_per_class=max_files_per_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vvqtZboVGBHi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def remove_ipynb_checkpoints(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        if '.ipynb_checkpoints' in dirs:\n",
        "            checkpoint_path = os.path.join(root, '.ipynb_checkpoints')\n",
        "            shutil.rmtree(checkpoint_path)\n",
        "            print(f\"Removed: {checkpoint_path}\")\n",
        "\n",
        "# Ana dizinler\n",
        "source_directory = '/C:/Users/asil_/Downloads/archive/ECG_Image_data'\n",
        "train_directory = '/C:/Users/asil_/Downloads/archive/ECG_Image_data_balanced/train'\n",
        "val_directory = '/C:/Users/asil_/Downloads/archive/ECG_Image_data_balanced/val'\n",
        "test_directory = 'C:/Users/asil_/Downloads/archive/ECG_Image_data_balanced/test'\n",
        "\n",
        "# Tüm dizinlerde .ipynb_checkpoints klasörlerini sil\n",
        "remove_ipynb_checkpoints(source_directory)\n",
        "remove_ipynb_checkpoints(train_directory)\n",
        "remove_ipynb_checkpoints(val_directory)\n",
        "remove_ipynb_checkpoints(test_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rR5ySc1kamdq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 corrupted files removed.\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def find_corrupted_images(directory):\n",
        "    corrupted_files = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            try:\n",
        "                img = Image.open(file_path)\n",
        "                img.verify()  # Verify that it is, in fact, an image\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(f\"Corrupted image detected: {file_path}\")\n",
        "                corrupted_files.append(file_path)\n",
        "    return corrupted_files\n",
        "\n",
        "train_dir = train_directory\n",
        "val_dir = val_directory \n",
        "test_dir = test_directory \n",
        "\n",
        "# Tüm veri setini kontrol edin\n",
        "corrupted_files = find_corrupted_images(train_dir) + find_corrupted_images(val_dir) + find_corrupted_images(test_dir)\n",
        "\n",
        "# Bozuk dosyaları temizle\n",
        "for file_path in corrupted_files:\n",
        "    os.remove(file_path)\n",
        "\n",
        "print(f\"{len(corrupted_files)} corrupted files removed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "rLiIQ-dEAZWS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: {}\n",
            "Test: {'F': 0, 'M': 298, 'N': 300, 'new_test': 0, 'Q': 296, 'S': 293, 'test': 0, 'train': 0, 'V': 299, 'val': 0}\n",
            "Val: {}\n"
          ]
        }
      ],
      "source": [
        "train_image_count = count_images_in_directory(train_dir)\n",
        "test_image_count = count_images_in_directory(test_dir)\n",
        "val_image_count = count_images_in_directory(val_dir)\n",
        "\n",
        "print(f\"Train: {train_image_count}\")\n",
        "print(f\"Test: {test_image_count}\")\n",
        "print(f\"Val: {val_image_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "S0PXO0hNAvsU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Directory:\n",
            "\n",
            "Test Directory:\n",
            "F: 0 images\n",
            "M: 298 images\n",
            "N: 300 images\n",
            "new_test: 0 images\n",
            "Q: 296 images\n",
            "S: 293 images\n",
            "test: 0 images\n",
            "train: 0 images\n",
            "V: 299 images\n",
            "val: 0 images\n",
            "\n",
            "Val Directory:\n",
            "Train: 0 images (0.00%)\n",
            "Test: 1486 images (100.00%)\n",
            "Val: 0 images (0.00%)\n"
          ]
        }
      ],
      "source": [
        "train_image_count = print_image_counts(train_dir, \"Train\")\n",
        "test_image_count = print_image_counts(test_dir, \"Test\")\n",
        "val_image_count = print_image_counts(val_dir, \"Val\")\n",
        "\n",
        "total_images = train_image_count + test_image_count + val_image_count\n",
        "\n",
        "train_percentage = (train_image_count / total_images) * 100\n",
        "test_percentage = (test_image_count / total_images) * 100\n",
        "val_percentage = (val_image_count / total_images) * 100\n",
        "\n",
        "print(f\"Train: {train_image_count} images ({train_percentage:.2f}%)\")\n",
        "print(f\"Test: {test_image_count} images ({test_percentage:.2f}%)\")\n",
        "print(f\"Val: {val_image_count} images ({val_percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSdBow1aVZbY"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Resmin yolu\n",
        "image_path = '/content/ECG_Image_data_balanced/train/F/F0.png'\n",
        "\n",
        "# Resmi aç\n",
        "img = Image.open(image_path)\n",
        "\n",
        "# Resmin boyutlarını al (genişlik, yükseklik)\n",
        "width, height = img.size\n",
        "\n",
        "print(f\"Resmin genişliği: {width} piksel\")\n",
        "print(f\"Resmin yüksekliği: {height} piksel\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YsjXRoXr4Q6"
      },
      "source": [
        "# Train & Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "tMEP13hlqesc",
        "outputId": "e5244cba-1a59-4c39-fc9b-b31f2961b024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_dir' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 37>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m test_val_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Directories for training, validation, and test sets\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dir\u001b[49m\n\u001b[0;32m     38\u001b[0m val_dir \u001b[38;5;241m=\u001b[39m val_dir\n\u001b[0;32m     39\u001b[0m test_dir \u001b[38;5;241m=\u001b[39m test_dir\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_dir' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Enable GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"Using GPU devices: {gpus}\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# ImageDataGenerator for data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Directories for training, validation, and test sets\n",
        "train_dir = train_dir\n",
        "val_dir = val_dir\n",
        "test_dir = test_dir\n",
        "\n",
        "# Flow data from directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(432, 224),  # Resmin genişliği: 432 piksel, yüksekliği: 288 piksel\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = test_val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(432, 288),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_val_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(432, 288),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # Important for getting correct labels when predicting\n",
        ")\n",
        "\n",
        "# Load VGG16 model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(432, 288, 3))\n",
        "\n",
        "# Freeze the first 15 layers of VGG16 to prevent them from being trained\n",
        "for layer in base_model.layers[:10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Keep the rest of the layers trainable\n",
        "for layer in base_model.layers[10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Add custom layers on top\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0001)  # VGG16 için daha düşük bir öğrenme oranı\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Directory for results\n",
        "results_dir = 'training_results_vgg16_6_classes_1'\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "checkpoint_path = os.path.join(results_dir, 'best_model_vgg16.keras')\n",
        "checkpoint = ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, mode='min')\n",
        "\n",
        "# Learning rate reduction\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Save the training history\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.to_csv(os.path.join(results_dir, 'training_history_vgg16.csv'))\n",
        "\n",
        "# Evaluate the model on train data\n",
        "train_generator.reset()\n",
        "Y_train_pred = model.predict(train_generator)\n",
        "Y_train_pred_classes = np.argmax(Y_train_pred, axis=1)\n",
        "y_train_classes = train_generator.classes  # True labels\n",
        "\n",
        "# Confusion matrix and classification report for train data\n",
        "confusion_mtx_train = confusion_matrix(y_train_classes, Y_train_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx_train, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Train Data)')\n",
        "plt.savefig(os.path.join(results_dir, 'confusion_matrix_train_vgg16.png'))\n",
        "plt.close()\n",
        "\n",
        "report_train = classification_report(y_train_classes, Y_train_pred_classes, target_names=list(train_generator.class_indices.keys()))\n",
        "with open(os.path.join(results_dir, 'classification_report_train_vgg16.txt'), 'w') as f:\n",
        "    f.write(report_train)\n",
        "\n",
        "# Evaluate the model on validation data\n",
        "val_generator.reset()\n",
        "Y_val_pred = model.predict(val_generator)\n",
        "Y_val_pred_classes = np.argmax(Y_val_pred, axis=1)\n",
        "y_val_classes = val_generator.classes  # True labels\n",
        "\n",
        "# Confusion matrix and classification report for validation data\n",
        "confusion_mtx_val = confusion_matrix(y_val_classes, Y_val_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx_val, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Validation Data)')\n",
        "plt.savefig(os.path.join(results_dir, 'confusion_matrix_val_vgg16.png'))\n",
        "plt.close()\n",
        "\n",
        "report_val = classification_report(y_val_classes, Y_val_pred_classes, target_names=list(val_generator.class_indices.keys()))\n",
        "with open(os.path.join(results_dir, 'classification_report_val_vgg16.txt'), 'w') as f:\n",
        "    f.write(report_val)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_generator.reset()\n",
        "Y_pred = model.predict(test_generator)\n",
        "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "y_test_classes = test_generator.classes  # True labels\n",
        "\n",
        "# Handle the case where there are missing classes in predictions\n",
        "unique_true_labels = np.unique(y_test_classes)\n",
        "unique_pred_labels = np.unique(Y_pred_classes)\n",
        "\n",
        "# Tüm sınıflar (6 sınıf olduğunu varsayıyoruz)\n",
        "all_classes = np.array(range(len(test_generator.class_indices)))\n",
        "\n",
        "# Eksik sınıfları doldur\n",
        "missing_true_labels = np.setdiff1d(all_classes, unique_true_labels)\n",
        "missing_pred_labels = np.setdiff1d(all_classes, unique_pred_labels)\n",
        "\n",
        "# Gerçek sınıflara eksik sınıfları ekle\n",
        "for label in missing_true_labels:\n",
        "    y_test_classes = np.append(y_test_classes, label)\n",
        "    Y_pred_classes = np.append(Y_pred_classes, label)\n",
        "\n",
        "# Tahmin edilen sınıflara eksik sınıfları ekle\n",
        "for label in missing_pred_labels:\n",
        "    Y_pred_classes = np.append(Y_pred_classes, label)\n",
        "    y_test_classes = np.append(y_test_classes, label)\n",
        "\n",
        "# Confusion matrix and classification report for test data\n",
        "confusion_mtx = confusion_matrix(y_test_classes, Y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Test Data)')\n",
        "plt.savefig(os.path.join(results_dir, 'confusion_matrix_vgg16.png'))\n",
        "plt.close()\n",
        "\n",
        "report = classification_report(y_test_classes, Y_pred_classes, target_names=list(test_generator.class_indices.keys()))\n",
        "with open(os.path.join(results_dir, 'classification_report_vgg16.txt'), 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(\"Model training and evaluation complete. Results saved in 'training_results_vgg16_6_classes' directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRMW03cxBR-d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "authorship_tag": "ABX9TyPBdCMotU4yKC1K7GYUL+/d",
      "gpuType": "V28",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
